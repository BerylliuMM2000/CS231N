{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class SkiDataset(Dataset):\n",
    "    '''2D alpine skiing dataset'''\n",
    "\n",
    "    # BGR channel-wise mean and std\n",
    "    all_mean = torch.Tensor([190.24553031, 176.98437134, 170.87045832]) / 255\n",
    "    all_std  = torch.Tensor([ 36.57356531,  35.29007466,  36.28703238]) / 255\n",
    "    train_mean = torch.Tensor([190.37117484, 176.86400202, 170.65409075]) / 255\n",
    "    train_std  = torch.Tensor([ 36.56829177,  35.27981661,  36.19375109]) / 255\n",
    "\n",
    "    # Corresponding joints\n",
    "    joints = ['head', 'neck',\n",
    "              'shoulder_right', 'elbow_right', 'hand_right', 'pole_basket_right',\n",
    "              'shoulder_left', 'elbow_left', 'hand_left', 'pole_basket_left',\n",
    "              'hip_right', 'knee_right', 'ankle_right',\n",
    "              'hip_left', 'knee_left', 'ankle_left',\n",
    "              'ski_tip_right', 'toes_right', 'heel_right', 'ski_tail_right',\n",
    "              'ski_tip_left', 'toes_left', 'heel_left', 'ski_tail_left']\n",
    "\n",
    "    # Bones for drawing examples\n",
    "    bones = [[0,1], [1,2], [2,3], [3,4], [4,5], [1,6], [6,7], [7,8], [8,9],\n",
    "             [2,10], [10,11], [11,12], [6,13], [13,14], [14,15],\n",
    "             [16,17], [17,18], [18,19], [12,17], [12,18],\n",
    "             [20,21], [21,22], [22,23], [15,21], [15,22]]\n",
    "\n",
    "    # VideoIDs and SplitIDs used for validation\n",
    "    val_splits = [('5UHRvqx1iuQ', '0'), ('5UHRvqx1iuQ', '1'),\n",
    "                  ('oKQFABiOTw8', '0'), ('oKQFABiOTw8', '1'), ('oKQFABiOTw8', '2'),\n",
    "                  ('qxfgw1Kd98A', '0'), ('qxfgw1Kd98A', '1'),\n",
    "                  ('uLW74013Wp0', '0'), ('uLW74013Wp0', '1'),\n",
    "                  ('zW1bF2PsB0M', '0'), ('zW1bF2PsB0M', '1')]\n",
    "\n",
    "\n",
    "    def __init__(self, imgs_dir, label_path, target_path, img_extension='png', mode='all', img_size=(1920,1080),\n",
    "                 normalize=True, in_pixels=True, return_info=False, transform=None):\n",
    "        '''\n",
    "        Create a Ski2DPose dataset loading train or validation images.\n",
    "\n",
    "        Args:\n",
    "            :imgs_dir: Root directory where images are saved\n",
    "            :label_path: Path to label JSON file\n",
    "            :img_extension: Image format extension depending on downloaded version. One of {'png', 'jpg', 'webp'}\n",
    "            :mode: Specify which partition to load. One of {'train', 'val', 'all'}\n",
    "            :img_size: Size of images to return\n",
    "            :normalize: Set to True to normalize images\n",
    "            :in_pixels: Set to True to scale annotations to pixels\n",
    "            :return_info: Set to True to include image names when getting items\n",
    "        '''\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.img_extension = img_extension\n",
    "        self.mode = mode\n",
    "        self.img_size = img_size\n",
    "        self.normalize = normalize\n",
    "        self.in_pixels = in_pixels\n",
    "        self.return_info = return_info\n",
    "        self.transform = transform # Added\n",
    "\n",
    "        assert mode in ['train', 'val', 'all'], 'Please select a valid mode.'\n",
    "        self.mean = self.all_mean if self.mode == 'all' else self.train_mean\n",
    "        self.std = self.all_std if self.mode == 'all' else self.train_std\n",
    "\n",
    "        # Load annotations\n",
    "        with open(label_path) as f:\n",
    "            self.labels = json.load(f)\n",
    "\n",
    "        # Added: load box information\n",
    "        with open(target_path) as g:\n",
    "            self.target = json.load(g)\n",
    "\n",
    "        # Check if all images exist and index them\n",
    "        self.index_list = []\n",
    "        for video_id, all_splits in self.labels.items():\n",
    "            for split_id, split in all_splits.items():\n",
    "                for img_id, img_labels in split.items():\n",
    "                    img_path = os.path.join(imgs_dir, video_id, split_id, '{}.{}'.format(img_id, img_extension))\n",
    "                    if os.path.exists(img_path):\n",
    "                        if ((mode == 'all') or\n",
    "                            (mode == 'train' and (video_id, split_id) not in self.val_splits) or\n",
    "                            (mode == 'val' and (video_id, split_id) in self.val_splits)):\n",
    "                            self.index_list.append((video_id, split_id, img_id))\n",
    "                    else:\n",
    "                        print('Did not find image {}/{}/{}.{}'.format(video_id, split_id, img_id, img_extension))\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Returns the number of samples in the dataset'''\n",
    "        return len(self.index_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns image tensor (H x W x C) in BGR format with values between 0 and 255,\n",
    "        annotation tensor (J x 2) and visibility flag tensor (J).\n",
    "\n",
    "        If 'normalize' flag was set to True, returned image will be normalized\n",
    "        according to mean and std above.\n",
    "\n",
    "        If 'return_info' flag was set to True, returns (video_id, split_id, img_id, frame_idx)\n",
    "        in addition.\n",
    "\n",
    "        Args:\n",
    "            :index: Index of data sample to load\n",
    "        '''\n",
    "        # Load annotations\n",
    "        video_id, split_id, img_id = self.index_list[index] \n",
    "        annotation = self.labels[video_id][split_id][img_id]['annotation']\n",
    "        frame_idx  = self.labels[video_id][split_id][img_id]['frame_idx']\n",
    "        boxes = self.target[video_id][split_id][img_id][\"boxes\"]  # Load box information\n",
    "        an = torch.Tensor(annotation)[:,:2]\n",
    "        vis = torch.LongTensor(annotation)[:,2]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.imgs_dir, video_id, split_id, '{}.{}'.format(img_id, self.img_extension))\n",
    "        img = cv2.imread(img_path) # (H x W x C) BGR\n",
    "        if self.img_size is not None:\n",
    "            img = cv2.resize(img, self.img_size)\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        if self.normalize:\n",
    "            img = ((img / 255) - self.mean) / self.std\n",
    "        if self.in_pixels:\n",
    "            an *= torch.Tensor([img.shape[1], img.shape[0]])\n",
    "\n",
    "        # NEW : Prepare the final target directory\n",
    "        target = {}\n",
    "        # Concat annotation and visibility together to return _*_*3 tensor\n",
    "        target[\"labels\"] = torch.cat((an, vis.unsqueeze(1)), 1)\n",
    "        target[\"boxes\"] = torch.Tensor(boxes)\n",
    "\n",
    "        if self.return_info:\n",
    "            return img, target, (video_id, split_id, img_id, frame_idx)\n",
    "        \n",
    "        # Added: reorder dimensions\n",
    "        img = img.permute(2, 0, 1)\n",
    "\n",
    "        # Added\n",
    "        if self.transform:\n",
    "            img = self.transform(img)  # Apply the transform to the image\n",
    "\n",
    "        # return img, an, vis\n",
    "        return img, target\n",
    "\n",
    "    def annotate_img(self, img, an, vis, info=None):\n",
    "        '''\n",
    "        Annotates a given image with all joints. Visible joints will be drawn\n",
    "        with a red circle, while invisible ones with a blue one.\n",
    "\n",
    "        Args:\n",
    "            :img: Input image (in pixels)\n",
    "            :an: Annotation positions tensor (in pixels)\n",
    "            :vis: Visibility flag tensor\n",
    "            :info: (video_id, split_id, img_id, frame_idx) tuple\n",
    "        '''\n",
    "        width, height = img.shape[1], img.shape[0]\n",
    "        img = img.numpy()\n",
    "        # Scale based on head-foot distance\n",
    "        scale = torch.norm(an[0] - an[15]) / height\n",
    "        img_an = img.copy()\n",
    "        # Draw all bones\n",
    "        for bone_from, bone_to in self.bones:\n",
    "            x_from, y_from = an[bone_from]\n",
    "            x_to, y_to = an[bone_to]\n",
    "            # MODIFIED: Cast x_from and y_from to int to avoid cv2 error\n",
    "            cv2.line(img_an, (int(x_from), int(y_from)), (int(x_to), int(y_to)), (0,255,0), int(max(2,5*scale)))\n",
    "        # Draw all joints\n",
    "        for (x,y), flag in zip(an, vis):\n",
    "            color = (0,0,255) if flag == 1 else (255,0,0)\n",
    "            # MODIFIED: Cast x and y to int to avoid cv2 error\n",
    "            cv2.circle(img_an, (int(x),int(y)), int(max(2,14*scale)), color, -1)\n",
    "        # Draw image name and frame number if given\n",
    "        if info is not None:\n",
    "            text = 'Image {}, frame {}.'.format(info[2], info[3])\n",
    "            cv2.putText(img_an, text, (5,40), cv2.FONT_HERSHEY_SIMPLEX, 1.5*(width/1920),\n",
    "                        (0,0,0), 5, cv2.LINE_AA)\n",
    "            cv2.putText(img_an, text, (5,40), cv2.FONT_HERSHEY_SIMPLEX, 1.5*(width/1920),\n",
    "                        (255,255,255), 2, cv2.LINE_AA)\n",
    "        return img_an\n",
    "\n",
    "\n",
    "def determine_image_format():\n",
    "    \"\"\"\n",
    "    @return (image_format_name, image_directory)\n",
    "    \"\"\"\n",
    "    formats = ['png', 'webp', 'jpg']\n",
    "\n",
    "    for img_format in formats:\n",
    "        img_dir = Path(f'Images_{img_format}')\n",
    "        if img_dir.is_dir():\n",
    "            print(f'Found image directory {img_dir}, using {img_format} format.')\n",
    "            return img_format, img_dir\n",
    "    \n",
    "    raise FileNotFoundError('Image directory not found, please ensure one of the following directories exists: ' + ', '.join(f'Images_{ext}' for ext in formats))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example dataloader for 2D alpine ski dataset\n",
      "Found image directory Images_webp, using webp format.\n",
      "Number of images: 1830\n"
     ]
    }
   ],
   "source": [
    "print('Example dataloader for 2D alpine ski dataset')\n",
    "\n",
    "transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), ])\n",
    "\n",
    "label_path = './ski2dpose_labels.json'\n",
    "target_path = './target.json'\n",
    "img_extension, imgs_dir = determine_image_format()\n",
    "\n",
    "train_dataset = SkiDataset(imgs_dir=imgs_dir, label_path=label_path, target_path=target_path, img_extension=img_extension,\n",
    "                        img_size=(1920,1080), mode='train', normalize=False, in_pixels=True, return_info=False,\n",
    "                        transform=transform)\n",
    "val_dataset = SkiDataset(imgs_dir=imgs_dir, label_path=label_path, target_path=target_path, img_extension=img_extension,\n",
    "                        img_size=(1920,1080), mode='val', normalize=False, in_pixels=True, return_info=False,\n",
    "                        transform = transform)\n",
    "print('Number of images: {}'.format(len(train_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25674\\AppData\\Local\\Temp/ipykernel_12112/3904186621.py:122: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  vis = torch.LongTensor(annotation)[:,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Image data reshaped to: torch.Size([3, 1080, 1920])\n",
      "24 Joint locations and visibility torch.Size([24, 3])\n",
      "This is the bounding box: tensor([0.4801, 0.2005, 0.7296, 0.7404])\n"
     ]
    }
   ],
   "source": [
    "print(\"1: Image data reshaped to:\", train_dataset[0][0].size())\n",
    "# print(\"2: 24 Joint locations, each indicated by xy coordinates, size:\", train_dataset[0][1])\n",
    "print(\"24 Joint locations and visibility\", train_dataset[0][1][\"labels\"].size())\n",
    "print(\"This is the bounding box:\", train_dataset[0][1][\"boxes\"])\n",
    "# print(\"3: Visibility of 24 joints\", train_dataset[0][2])\n",
    "# print(\"4: Inforamtion: video id; split id; image id; frame idx\", train_dataset[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25674\\AppData\\Local\\Temp/ipykernel_12112/3904186621.py:122: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  vis = torch.LongTensor(annotation)[:,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([5, 3, 1080, 1920])\n",
      "Keypoints shape: torch.Size([5, 24, 3])\n",
      "Boxes shape: torch.Size([5, 4])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12112/2221539500.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     '''\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mloss_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeypoint_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Check for degenerate boxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\detection\\transform.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m             \u001b[0mtarget_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "def get_model():\n",
    "    # initialize the model\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(num_keypoints=24)\n",
    "    return model\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "keypoint_model = get_model().to(DEVICE).eval()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(keypoint_model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  keypoint_model.train()\n",
    "  for i, (images, labels_raw) in enumerate(train_dataloader): \n",
    "    print(\"Image shape:\", images.shape)\n",
    "    print(\"Keypoints shape:\", labels_raw[\"labels\"].shape)\n",
    "    print(\"Boxes shape:\", labels_raw[\"boxes\"].shape)\n",
    "    optimizer.zero_grad()\n",
    "    images = list(image.to(DEVICE) for image in images)\n",
    "\n",
    "    # The model expects targets to be a list of dictionary\n",
    "    # https://debuggercafe.com/a-simple-pipeline-to-train-pytorch-faster-rcnn-object-detection-model/\n",
    "    #### Thus we want labels = [{\"labels\": ...., \"boxes\":....}]\n",
    "    labels = [{\"keypoints\": labels_raw[\"labels\"].to(DEVICE), \"boxes\": labels_raw[\"boxes\"].to(DEVICE)}]\n",
    "\n",
    "    '''\n",
    "    During training, the model expects both the input tensors and targets (list of dictionary),\n",
    "    containing:\n",
    "        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n",
    "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
    "        - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
    "\n",
    "    The model returns a Dict[Tensor] during training, containing the classification and regression\n",
    "    losses for both the RPN and the R-CNN.\n",
    "\n",
    "    During inference, the model requires only the input tensors, and returns the post-processed\n",
    "    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
    "    follows:\n",
    "        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n",
    "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
    "        - labels (Int64Tensor[N]): the predicted labels for each image\n",
    "        - scores (Tensor[N]): the scores or each prediction\n",
    "\n",
    "    '''\n",
    "    loss_dict = keypoint_model(images, labels)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "    loss_value = losses.item()\n",
    "    losses.backward()\n",
    "\n",
    "    #losses = criterion(outputs, labels)\n",
    "    #loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    '''\n",
    "    keypoint_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for images, labels, _ in test_dataloader:\n",
    "            outputs = keypoint_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "        valid_loss = valid_loss / len(test_dataloader)\n",
    "        print(f\"Epoch: {epoch + 1}, Validation Loss: {valid_loss:.4f}\")\n",
    "        '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25674\\AppData\\Local\\Temp/ipykernel_12112/3904186621.py:122: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  vis = torch.LongTensor(annotation)[:,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([5, 3, 1080, 1920])\n",
      "Labels shape: torch.Size([5, 24, 3])\n",
      "Boxes shape: torch.Size([5, 4])\n",
      "[{'labels': [tensor([[1.3177e+03, 5.1922e+02, 1.0000e+00],\n",
      "        [1.3296e+03, 5.5804e+02, 1.0000e+00],\n",
      "        [1.3575e+03, 5.6102e+02, 1.0000e+00],\n",
      "        [1.3634e+03, 5.9187e+02, 0.0000e+00],\n",
      "        [1.3177e+03, 5.7097e+02, 0.0000e+00],\n",
      "        [1.4112e+03, 6.7945e+02, 0.0000e+00],\n",
      "        [1.3037e+03, 5.7894e+02, 1.0000e+00],\n",
      "        [1.2629e+03, 5.9984e+02, 1.0000e+00],\n",
      "        [1.2490e+03, 5.8391e+02, 1.0000e+00],\n",
      "        [1.1763e+03, 7.2722e+02, 1.0000e+00],\n",
      "        [1.3953e+03, 6.5059e+02, 1.0000e+00],\n",
      "        [1.3515e+03, 6.4661e+02, 0.0000e+00],\n",
      "        [1.4490e+03, 6.9737e+02, 1.0000e+00],\n",
      "        [1.3634e+03, 6.6154e+02, 1.0000e+00],\n",
      "        [1.3207e+03, 6.5358e+02, 1.0000e+00],\n",
      "        [1.3774e+03, 6.9737e+02, 1.0000e+00],\n",
      "        [1.3455e+03, 6.6950e+02, 0.0000e+00],\n",
      "        [1.4132e+03, 6.9040e+02, 0.0000e+00],\n",
      "        [1.4779e+03, 7.0931e+02, 1.0000e+00],\n",
      "        [1.5326e+03, 7.2523e+02, 1.0000e+00],\n",
      "        [1.2908e+03, 6.7945e+02, 1.0000e+00],\n",
      "        [1.3605e+03, 7.0035e+02, 1.0000e+00],\n",
      "        [1.4202e+03, 7.1628e+02, 1.0000e+00],\n",
      "        [1.4869e+03, 7.3319e+02, 1.0000e+00]]), tensor([[1.0978e+03, 4.3256e+02, 1.0000e+00],\n",
      "        [1.1046e+03, 4.9449e+02, 1.0000e+00],\n",
      "        [1.0615e+03, 4.8543e+02, 1.0000e+00],\n",
      "        [9.9960e+02, 4.7259e+02, 1.0000e+00],\n",
      "        [9.2710e+02, 5.3603e+02, 1.0000e+00],\n",
      "        [9.3314e+02, 5.5416e+02, 1.0000e+00],\n",
      "        [1.1688e+03, 5.2923e+02, 1.0000e+00],\n",
      "        [1.1884e+03, 6.2289e+02, 1.0000e+00],\n",
      "        [1.1733e+03, 6.9766e+02, 1.0000e+00],\n",
      "        [1.1892e+03, 6.7651e+02, 1.0000e+00],\n",
      "        [1.0646e+03, 5.8739e+02, 1.0000e+00],\n",
      "        [9.5504e+02, 6.4932e+02, 1.0000e+00],\n",
      "        [8.3873e+02, 7.0370e+02, 1.0000e+00],\n",
      "        [1.0985e+03, 6.3573e+02, 1.0000e+00],\n",
      "        [1.0933e+03, 6.1231e+02, 1.0000e+00],\n",
      "        [9.9356e+02, 6.8935e+02, 1.0000e+00],\n",
      "        [7.3979e+02, 7.3920e+02, 1.0000e+00],\n",
      "        [8.0776e+02, 7.2107e+02, 1.0000e+00],\n",
      "        [8.3571e+02, 7.1503e+02, 1.0000e+00],\n",
      "        [8.6365e+02, 7.1352e+02, 1.0000e+00],\n",
      "        [9.0897e+02, 7.3618e+02, 1.0000e+00],\n",
      "        [9.5882e+02, 7.1503e+02, 1.0000e+00],\n",
      "        [9.8827e+02, 7.0446e+02, 1.0000e+00],\n",
      "        [1.0124e+03, 6.9388e+02, 1.0000e+00]]), tensor([[1.4154e+03, 2.6008e+02, 1.0000e+00],\n",
      "        [1.4315e+03, 3.1236e+02, 1.0000e+00],\n",
      "        [1.4023e+03, 3.2040e+02, 1.0000e+00],\n",
      "        [1.3752e+03, 3.5156e+02, 1.0000e+00],\n",
      "        [1.3320e+03, 3.6061e+02, 1.0000e+00],\n",
      "        [1.2857e+03, 4.9330e+02, 1.0000e+00],\n",
      "        [1.4687e+03, 3.1839e+02, 1.0000e+00],\n",
      "        [1.4818e+03, 3.7066e+02, 1.0000e+00],\n",
      "        [1.4235e+03, 3.8373e+02, 1.0000e+00],\n",
      "        [1.6728e+03, 4.2294e+02, 1.0000e+00],\n",
      "        [1.4707e+03, 4.1288e+02, 1.0000e+00],\n",
      "        [1.4275e+03, 4.4606e+02, 1.0000e+00],\n",
      "        [1.4938e+03, 5.3854e+02, 1.0000e+00],\n",
      "        [1.4989e+03, 4.0484e+02, 1.0000e+00],\n",
      "        [1.4787e+03, 4.4907e+02, 1.0000e+00],\n",
      "        [1.5552e+03, 5.2849e+02, 1.0000e+00],\n",
      "        [1.3641e+03, 5.7674e+02, 1.0000e+00],\n",
      "        [1.4667e+03, 5.6669e+02, 1.0000e+00],\n",
      "        [1.5240e+03, 5.5664e+02, 1.0000e+00],\n",
      "        [1.5753e+03, 5.3955e+02, 0.0000e+00],\n",
      "        [1.4225e+03, 5.7473e+02, 1.0000e+00],\n",
      "        [1.5290e+03, 5.5664e+02, 1.0000e+00],\n",
      "        [1.5903e+03, 5.4256e+02, 1.0000e+00],\n",
      "        [1.6768e+03, 5.1542e+02, 1.0000e+00]]), tensor([[1.1631e+03, 2.1494e+02, 1.0000e+00],\n",
      "        [1.1615e+03, 3.3707e+02, 1.0000e+00],\n",
      "        [1.0618e+03, 3.3209e+02, 1.0000e+00],\n",
      "        [9.9945e+02, 4.2264e+02, 1.0000e+00],\n",
      "        [1.0618e+03, 5.1985e+02, 1.0000e+00],\n",
      "        [6.1562e+02, 6.0459e+02, 1.0000e+00],\n",
      "        [1.2587e+03, 3.6200e+02, 1.0000e+00],\n",
      "        [1.2612e+03, 4.6335e+02, 1.0000e+00],\n",
      "        [1.2528e+03, 6.4114e+02, 1.0000e+00],\n",
      "        [1.1174e+03, 6.4031e+02, 1.0000e+00],\n",
      "        [1.0651e+03, 4.9492e+02, 1.0000e+00],\n",
      "        [9.6622e+02, 5.9794e+02, 1.0000e+00],\n",
      "        [7.9591e+02, 6.7355e+02, 1.0000e+00],\n",
      "        [1.1615e+03, 5.2566e+02, 1.0000e+00],\n",
      "        [1.1872e+03, 5.0572e+02, 1.0000e+00],\n",
      "        [1.0468e+03, 6.4114e+02, 1.0000e+00],\n",
      "        [8.3828e+02, 7.3586e+02, 1.0000e+00],\n",
      "        [7.5271e+02, 7.4084e+02, 1.0000e+00],\n",
      "        [7.0369e+02, 7.3419e+02, 1.0000e+00],\n",
      "        [6.4969e+02, 7.2672e+02, 0.0000e+00],\n",
      "        [1.0991e+03, 7.0761e+02, 1.0000e+00],\n",
      "        [1.0177e+03, 7.1342e+02, 1.0000e+00],\n",
      "        [9.6124e+02, 7.1010e+02, 1.0000e+00],\n",
      "        [8.9976e+02, 7.0096e+02, 0.0000e+00]]), tensor([[1.0363e+03, 1.2033e+02, 1.0000e+00],\n",
      "        [9.8872e+02, 2.2600e+02, 1.0000e+00],\n",
      "        [9.2260e+02, 2.5319e+02, 1.0000e+00],\n",
      "        [9.3990e+02, 3.6442e+02, 1.0000e+00],\n",
      "        [1.0864e+03, 4.0211e+02, 1.0000e+00],\n",
      "        [7.2609e+02, 6.7463e+02, 1.0000e+00],\n",
      "        [1.0524e+03, 2.5319e+02, 1.0000e+00],\n",
      "        [1.1346e+03, 3.1807e+02, 1.0000e+00],\n",
      "        [1.1438e+03, 3.3723e+02, 1.0000e+00],\n",
      "        [1.4534e+03, 6.5918e+02, 1.0000e+00],\n",
      "        [8.5772e+02, 4.4660e+02, 1.0000e+00],\n",
      "        [9.5103e+02, 5.2879e+02, 1.0000e+00],\n",
      "        [9.6153e+02, 7.4569e+02, 1.0000e+00],\n",
      "        [9.3558e+02, 4.4104e+02, 1.0000e+00],\n",
      "        [1.0103e+03, 5.3806e+02, 1.0000e+00],\n",
      "        [1.0239e+03, 7.4384e+02, 1.0000e+00],\n",
      "        [1.1889e+03, 8.2294e+02, 1.0000e+00],\n",
      "        [1.0073e+03, 8.0069e+02, 1.0000e+00],\n",
      "        [9.3867e+02, 7.9019e+02, 1.0000e+00],\n",
      "        [8.2373e+02, 7.7474e+02, 1.0000e+00],\n",
      "        [1.2637e+03, 8.1861e+02, 1.0000e+00],\n",
      "        [1.0796e+03, 7.9513e+02, 1.0000e+00],\n",
      "        [9.9737e+02, 7.8215e+02, 1.0000e+00],\n",
      "        [8.9974e+02, 7.6670e+02, 1.0000e+00]])], 'boxes': [tensor([0.5627, 0.4308, 0.8482, 0.7289]), tensor([0.3353, 0.3505, 0.6694, 0.7344]), tensor([0.6197, 0.1908, 0.9233, 0.5840]), tensor([0.2706, 0.1490, 0.7069, 0.7360]), tensor([0.3282, 0.0614, 0.8070, 0.8120])]}]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected target boxes to be of type Tensor, got <class 'list'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12112/2286230617.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     '''\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[0mloss_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeypoint_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     70\u001b[0m                         )\n\u001b[0;32m     71\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Expected target boxes to be of type Tensor, got {type(boxes)}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0moriginal_image_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_assert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1209\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[1;31m################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expected target boxes to be of type Tensor, got <class 'list'>."
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "def get_model():\n",
    "    # initialize the model\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(num_keypoints=24)\n",
    "    return model\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "keypoint_model = get_model().to(DEVICE).eval()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(keypoint_model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  keypoint_model.train()\n",
    "  for i, (images, labels_raw) in enumerate(train_dataloader): \n",
    "    print(\"Image shape:\", images.shape)\n",
    "    print(\"Labels shape:\", labels_raw[\"labels\"].shape)\n",
    "    print(\"Boxes shape:\", labels_raw[\"boxes\"].shape)\n",
    "    optimizer.zero_grad()\n",
    "    images = list(image.to(DEVICE) for image in images)\n",
    "    # labels_and_vis = []\n",
    "    # boxes = []\n",
    "    # for label in labels:\n",
    "    #    l = label[\"labels\"]\n",
    "    #    b = label[\"boxes\"]\n",
    "    #    labels_and_vis\n",
    "    # # labels = list(label.to(DEVICE) for label in labels)\n",
    "    # vis = list(v.to(DEVICE) for v in vis)\n",
    "    # images, labels, vis = images.to(DEVICE), labels.to(DEVICE), vis.to(DEVICE)\n",
    "    # labels_and_vis = torch.cat((labels, vis.unsqueeze(2)), 2)\n",
    "    labels = {}\n",
    "    for k, v in labels_raw.items():\n",
    "       v = [i.to(DEVICE) for i in v]\n",
    "       labels[k] = v\n",
    "    labels = [labels]\n",
    "    print(labels)\n",
    "    # labels = [{k: v.to(DEVICE) for k, v in labels.items()}]\n",
    "    # The model expects targets to be a list of dictionary\n",
    "    # https://debuggercafe.com/a-simple-pipeline-to-train-pytorch-faster-rcnn-object-detection-model/\n",
    "\n",
    "    '''\n",
    "    During training, the model expects both the input tensors and targets (list of dictionary),\n",
    "    containing:\n",
    "        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n",
    "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
    "        - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
    "\n",
    "    The model returns a Dict[Tensor] during training, containing the classification and regression\n",
    "    losses for both the RPN and the R-CNN.\n",
    "\n",
    "    During inference, the model requires only the input tensors, and returns the post-processed\n",
    "    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
    "    follows:\n",
    "        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n",
    "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
    "        - labels (Int64Tensor[N]): the predicted labels for each image\n",
    "        - scores (Tensor[N]): the scores or each prediction\n",
    "\n",
    "    '''\n",
    "    loss_dict = keypoint_model(images, labels)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "    loss_value = losses.item()\n",
    "    losses.backward()\n",
    "\n",
    "    #losses = criterion(outputs, labels)\n",
    "    #loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    '''\n",
    "    keypoint_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for images, labels, _ in test_dataloader:\n",
    "            outputs = keypoint_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "        valid_loss = valid_loss / len(test_dataloader)\n",
    "        print(f\"Epoch: {epoch + 1}, Validation Loss: {valid_loss:.4f}\")\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
