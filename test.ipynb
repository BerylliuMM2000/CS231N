{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image from PIL format to a PyTorch tensor and scales it to the range [0, 1]\n",
    "])\n",
    "\n",
    "class SkiDataset(Dataset):\n",
    "    '''2D alpine skiing dataset'''\n",
    "\n",
    "    # BGR channel-wise mean and std\n",
    "    all_mean = torch.Tensor([190.24553031, 176.98437134, 170.87045832]) / 255\n",
    "    all_std  = torch.Tensor([ 36.57356531,  35.29007466,  36.28703238]) / 255\n",
    "    train_mean = torch.Tensor([190.37117484, 176.86400202, 170.65409075]) / 255\n",
    "    train_std  = torch.Tensor([ 36.56829177,  35.27981661,  36.19375109]) / 255\n",
    "\n",
    "    # Corresponding joints\n",
    "    joints = ['head', 'neck',\n",
    "              'shoulder_right', 'elbow_right', 'hand_right', 'pole_basket_right',\n",
    "              'shoulder_left', 'elbow_left', 'hand_left', 'pole_basket_left',\n",
    "              'hip_right', 'knee_right', 'ankle_right',\n",
    "              'hip_left', 'knee_left', 'ankle_left',\n",
    "              'ski_tip_right', 'toes_right', 'heel_right', 'ski_tail_right',\n",
    "              'ski_tip_left', 'toes_left', 'heel_left', 'ski_tail_left']\n",
    "\n",
    "    # Bones for drawing examples\n",
    "    bones = [[0,1], [1,2], [2,3], [3,4], [4,5], [1,6], [6,7], [7,8], [8,9],\n",
    "             [2,10], [10,11], [11,12], [6,13], [13,14], [14,15],\n",
    "             [16,17], [17,18], [18,19], [12,17], [12,18],\n",
    "             [20,21], [21,22], [22,23], [15,21], [15,22]]\n",
    "\n",
    "    # VideoIDs and SplitIDs used for validation\n",
    "    val_splits = [('5UHRvqx1iuQ', '0'), ('5UHRvqx1iuQ', '1'),\n",
    "                  ('oKQFABiOTw8', '0'), ('oKQFABiOTw8', '1'), ('oKQFABiOTw8', '2'),\n",
    "                  ('qxfgw1Kd98A', '0'), ('qxfgw1Kd98A', '1'),\n",
    "                  ('uLW74013Wp0', '0'), ('uLW74013Wp0', '1'),\n",
    "                  ('zW1bF2PsB0M', '0'), ('zW1bF2PsB0M', '1')]\n",
    "\n",
    "\n",
    "    def __init__(self, imgs_dir, label_path, img_extension='png', mode='all', img_size=(1920,1080),\n",
    "                 normalize=True, in_pixels=True, return_info=False, transform=None):\n",
    "        '''\n",
    "        Create a Ski2DPose dataset loading train or validation images.\n",
    "\n",
    "        Args:\n",
    "            :imgs_dir: Root directory where images are saved\n",
    "            :label_path: Path to label JSON file\n",
    "            :img_extension: Image format extension depending on downloaded version. One of {'png', 'jpg', 'webp'}\n",
    "            :mode: Specify which partition to load. One of {'train', 'val', 'all'}\n",
    "            :img_size: Size of images to return\n",
    "            :normalize: Set to True to normalize images\n",
    "            :in_pixels: Set to True to scale annotations to pixels\n",
    "            :return_info: Set to True to include image names when getting items\n",
    "        '''\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.img_extension = img_extension\n",
    "        self.mode = mode\n",
    "        self.img_size = img_size\n",
    "        self.normalize = normalize\n",
    "        self.in_pixels = in_pixels\n",
    "        self.return_info = return_info\n",
    "        self.transform = transform # Added\n",
    "\n",
    "        assert mode in ['train', 'val', 'all'], 'Please select a valid mode.'\n",
    "        self.mean = self.all_mean if self.mode == 'all' else self.train_mean\n",
    "        self.std = self.all_std if self.mode == 'all' else self.train_std\n",
    "\n",
    "        # Load annotations\n",
    "        with open(label_path) as f:\n",
    "            self.labels = json.load(f)\n",
    "\n",
    "        # Check if all images exist and index them\n",
    "        self.index_list = []\n",
    "        for video_id, all_splits in self.labels.items():\n",
    "            for split_id, split in all_splits.items():\n",
    "                for img_id, img_labels in split.items():\n",
    "                    img_path = os.path.join(imgs_dir, video_id, split_id, '{}.{}'.format(img_id, img_extension))\n",
    "                    if os.path.exists(img_path):\n",
    "                        if ((mode == 'all') or\n",
    "                            (mode == 'train' and (video_id, split_id) not in self.val_splits) or\n",
    "                            (mode == 'val' and (video_id, split_id) in self.val_splits)):\n",
    "                            self.index_list.append((video_id, split_id, img_id))\n",
    "                    else:\n",
    "                        print('Did not find image {}/{}/{}.{}'.format(video_id, split_id, img_id, img_extension))\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Returns the number of samples in the dataset'''\n",
    "        return len(self.index_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns image tensor (H x W x C) in BGR format with values between 0 and 255,\n",
    "        annotation tensor (J x 2) and visibility flag tensor (J).\n",
    "\n",
    "        If 'normalize' flag was set to True, returned image will be normalized\n",
    "        according to mean and std above.\n",
    "\n",
    "        If 'return_info' flag was set to True, returns (video_id, split_id, img_id, frame_idx)\n",
    "        in addition.\n",
    "\n",
    "        Args:\n",
    "            :index: Index of data sample to load\n",
    "        '''\n",
    "        # Load annotations\n",
    "        video_id, split_id, img_id = self.index_list[index]\n",
    "        annotation = self.labels[video_id][split_id][img_id]['annotation']\n",
    "        frame_idx  = self.labels[video_id][split_id][img_id]['frame_idx']\n",
    "        an = torch.Tensor(annotation)[:,:2]\n",
    "        vis = torch.LongTensor(annotation)[:,2]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.imgs_dir, video_id, split_id, '{}.{}'.format(img_id, self.img_extension))\n",
    "        img = cv2.imread(img_path) # (H x W x C) BGR\n",
    "        if self.img_size is not None:\n",
    "            img = cv2.resize(img, self.img_size)\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        if self.normalize:\n",
    "            img = ((img / 255) - self.mean) / self.std\n",
    "        if self.in_pixels:\n",
    "            an *= torch.Tensor([img.shape[1], img.shape[0]])\n",
    "        if self.return_info:\n",
    "            return img, an, vis, (video_id, split_id, img_id, frame_idx)\n",
    "        # Added\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply the transform to the image\n",
    "\n",
    "        return img, an, vis\n",
    "\n",
    "    def annotate_img(self, img, an, vis, info=None):\n",
    "        '''\n",
    "        Annotates a given image with all joints. Visible joints will be drawn\n",
    "        with a red circle, while invisible ones with a blue one.\n",
    "\n",
    "        Args:\n",
    "            :img: Input image (in pixels)\n",
    "            :an: Annotation positions tensor (in pixels)\n",
    "            :vis: Visibility flag tensor\n",
    "            :info: (video_id, split_id, img_id, frame_idx) tuple\n",
    "        '''\n",
    "        width, height = img.shape[1], img.shape[0]\n",
    "        img = img.numpy()\n",
    "        # Scale based on head-foot distance\n",
    "        scale = torch.norm(an[0] - an[15]) / height\n",
    "        img_an = img.copy()\n",
    "        # Draw all bones\n",
    "        for bone_from, bone_to in self.bones:\n",
    "            x_from, y_from = an[bone_from]\n",
    "            x_to, y_to = an[bone_to]\n",
    "            # MODIFIED: Cast x_from and y_from to int to avoid cv2 error\n",
    "            cv2.line(img_an, (int(x_from), int(y_from)), (int(x_to), int(y_to)), (0,255,0), int(max(2,5*scale)))\n",
    "        # Draw all joints\n",
    "        for (x,y), flag in zip(an, vis):\n",
    "            color = (0,0,255) if flag == 1 else (255,0,0)\n",
    "            # MODIFIED: Cast x and y to int to avoid cv2 error\n",
    "            cv2.circle(img_an, (int(x),int(y)), int(max(2,14*scale)), color, -1)\n",
    "        # Draw image name and frame number if given\n",
    "        if info is not None:\n",
    "            text = 'Image {}, frame {}.'.format(info[2], info[3])\n",
    "            cv2.putText(img_an, text, (5,40), cv2.FONT_HERSHEY_SIMPLEX, 1.5*(width/1920),\n",
    "                        (0,0,0), 5, cv2.LINE_AA)\n",
    "            cv2.putText(img_an, text, (5,40), cv2.FONT_HERSHEY_SIMPLEX, 1.5*(width/1920),\n",
    "                        (255,255,255), 2, cv2.LINE_AA)\n",
    "        return img_an\n",
    "\n",
    "\n",
    "def determine_image_format():\n",
    "    \"\"\"\n",
    "    @return (image_format_name, image_directory)\n",
    "    \"\"\"\n",
    "    formats = ['png', 'webp', 'jpg']\n",
    "\n",
    "    for img_format in formats:\n",
    "        img_dir = Path(f'Images_{img_format}')\n",
    "        if img_dir.is_dir():\n",
    "            print(f'Found image directory {img_dir}, using {img_format} format.')\n",
    "            return img_format, img_dir\n",
    "    \n",
    "    raise FileNotFoundError('Image directory not found, please ensure one of the following directories exists: ' + ', '.join(f'Images_{ext}' for ext in formats))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example dataloader for 2D alpine ski dataset\n",
      "Found image directory Images_webp, using webp format.\n",
      "Number of images: 1830\n"
     ]
    }
   ],
   "source": [
    "print('Example dataloader for 2D alpine ski dataset')\n",
    "\n",
    "label_path = './ski2dpose_labels.json'\n",
    "img_extension, imgs_dir = determine_image_format()\n",
    "\n",
    "train_dataset = SkiDataset(imgs_dir=imgs_dir, label_path=label_path, img_extension=img_extension,\n",
    "                        img_size=(1920,1080), mode='train', normalize=False, in_pixels=True, return_info=False)\n",
    "val_dataset = SkiDataset(imgs_dir=imgs_dir, label_path=label_path, img_extension=img_extension,\n",
    "                        img_size=(1920,1080), mode='val', normalize=False, in_pixels=True, return_info=False)\n",
    "print('Number of images: {}'.format(len(train_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Image data 1080*1920*3 torch.Size([1080, 1920, 3])\n",
      "2: 24 Joint locations, each indicated by xy coordinates torch.Size([24, 2])\n",
      "3: Visibility of 24 joints tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25674\\AppData\\Local\\Temp/ipykernel_8256/3515158152.py:121: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  vis = torch.LongTensor(annotation)[:,2]\n"
     ]
    }
   ],
   "source": [
    "print(\"1: Image data 1080*1920*3\", train_dataset[0][0].size())\n",
    "print(\"2: 24 Joint locations, each indicated by xy coordinates\", train_dataset[0][1].size())\n",
    "print(\"3: Visibility of 24 joints\", train_dataset[0][2])\n",
    "# print(\"4: Inforamtion: video id; split id; image id; frame idx\", train_dataset[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25674\\AppData\\Local\\Temp/ipykernel_8256/3515158152.py:121: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  vis = torch.LongTensor(annotation)[:,2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25674\\AppData\\Local\\Temp/ipykernel_8256/3515158152.py:121: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  vis = torch.LongTensor(annotation)[:,2]\n"
     ]
    }
   ],
   "source": [
    "# Expect 3-tuple when iterating through the dataloader as we've specified info=False\n",
    "image, annotation, vis = next(iter(train_dataloader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test toy CNN model by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.middle(x1)\n",
    "        x3 = self.decoder(x2)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channels=3, out_channels=48)  # 24 joints * 2 coordinates\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25674\\AppData\\Local\\Temp/ipykernel_8256/3515158152.py:121: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  vis = torch.LongTensor(annotation)[:,2]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (unsigned char) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8256/1462541874.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8256/538158612.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmiddle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mx3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (unsigned char) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (images, labels, _) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for images, labels, _ in test_dataloader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "        valid_loss = valid_loss / len(test_dataloader)\n",
    "        print(f\"Epoch: {epoch + 1}, Validation Loss: {valid_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
